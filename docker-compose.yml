networks:
  abada-public:
    name: abada-public
    driver: bridge
  abada-internal:
    name: abada-internal
    driver: bridge
    internal: true

services:
  # OpenTelemetry Collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    container_name: otel-collector
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./docker/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro,z
    ports:
      - "4317:4317" # OTLP gRPC receiver
      - "4318:4318" # OTLP HTTP receiver
      - "8889:8889" # Prometheus metrics
    networks:
      - abada-internal
    restart: unless-stopped

  # Jaeger for distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jaeger
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - JAEGER_STORAGE_TYPE=memory
    ports:
      - "16686:16686" # Jaeger UI
      - "14250:14250" # Jaeger gRPC
    networks:
      - abada-internal
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:16686/",
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  # Prometheus for metrics storage
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--storage.tsdb.retention.time=15d"
      - "--web.enable-lifecycle"
    volumes:
      - ./docker/prometheus.yml:/etc/prometheus/prometheus.yml:ro,z
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - abada-internal
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:9090/-/healthy",
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  loki:
    user: "0:0"
    image: grafana/loki:2.9.1
    container_name: loki
    command: -config.file=/etc/loki/local-config.yaml -config.expand-env=true
    volumes:
      - ./docker/loki-config-prod.yaml:/etc/loki/local-config.yaml:ro,z
      - ./docker/loki-wal:/tmp/loki/wal
      - ./docker/loki-compactor:/loki/compactor
    ports:
      - "3100:3100"
    networks:
      - abada-internal
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:3100/ready",
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  # Promtail for log collection
  promtail:
    image: grafana/promtail:2.9.1
    container_name: promtail
    command: -config.file=/etc/promtail/config.yml
    volumes:
      - ./docker/promtail-config.yaml:/etc/promtail/config.yml:ro,z
      - ./logs:/var/log/abada:ro,z
    networks:
      - abada-internal
    restart: unless-stopped
    depends_on:
      loki:
        condition: service_healthy

  consul:
    image: hashicorp/consul:latest
    container_name: consul
    ports:
      - "8500:8500"
    networks:
      - abada-internal
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:8500/v1/status/leader",
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./docker/grafana/provisioning:/etc/grafana/provisioning:ro,z
      - ./docker/grafana/dashboards:/var/lib/grafana/dashboards:ro,z
    ports:
      - "3000:3000"
    networks:
      - abada-internal
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  # Traefik load balancer
  traefik:
    image: traefik:v2.11
    container_name: traefik
    # Run as root to ensure socket access
    user: root
    environment:
      - DOCKER_HOST=unix:///var/run/docker.sock
    command:
      - "--configfile=/etc/traefik/traefik.yml"
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:rw
      - ./docker/traefik/traefik.yml:/etc/traefik/traefik.yml:ro,z
      - ./docker/traefik/dynamic.yml:/etc/traefik/dynamic.yml:ro,z
      - ./docker/traefik/certs:/etc/traefik/certs:ro,z
    networks:
      - abada-public
      - abada-internal
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "traefik", "healthcheck", "--ping"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  prometheus_data:
  grafana_data:
